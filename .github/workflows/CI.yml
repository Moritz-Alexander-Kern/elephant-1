# This workflow will set up GitHub-hosted runners and install the required dependencies for elephant tests.
# On a pull requests and on pushes to master it will run different tests for elephant.

name: tests
# define events that trigger workflow 'tests'
on:
  workflow_dispatch: # enables manual triggering of workflow
    inputs:
      logLevel:
        description: 'Log level'
        required: true
        default: 'warning'
        type: choice
        options:
          - info
          - warning
          - debug

  pull_request:
    branches:
      - master
    types:
      #- assigned
      #- unassigned
      #- labeled
      #- unlabeled
      - opened
      #- edited
      #- closed
      - reopened
      - synchronize
      #- converted_to_draft
      #- ready_for_review
      #- locked
      #- unlocked
      #- review_requested
      #- review_request_removed
      #- auto_merge_enabled
      #- auto_merge_disabled

  push:
    branches:
      - master


permissions:
  # deployments permission to deploy GitHub pages website
  deployments: write
  # contents permission to update benchmark contents in gh-pages branch
  contents: write


# jobs define the steps that will be executed on the runner
jobs:

  # install dependencies and elephant with pip and run tests with pytest
  benchmark-pytest:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        # python versions for elephant: [3.8, 3.9, "3.10", 3.11]
        python-version: [3.11]
        # OS [ubuntu-latest, macos-latest, windows-latest]
        os: [ubuntu-latest]
      # do not cancel all in-progress jobs if any matrix job fails
      fail-fast: false

    steps:
      # used to reset cache every month
      - name: Get current year-month
        id: date
        run: echo "date=$(date +'%Y-%m')" >> $GITHUB_OUTPUT

      - uses: actions/checkout@v3

      - name: Set up Python ${{ matrix.python-version }}
        uses: actions/setup-python@v4
        with:
          python-version: ${{ matrix.python-version }}
          cache: 'pip'
          cache-dependency-path: '**/requirements.txt'

      - name: Cache test_env
        uses: actions/cache@v3
        with:
          path: /home/runner/work/elephant-1/elephant-1/.benchmarks
          # Look to see if there is a cache hit for the corresponding requirements files
          # cache will be reset on changes to any requirements or every month
          key: ${{ runner.os }}-venv-${{ hashFiles('**/requirements.txt') }}-${{ hashFiles('**/requirements-tests.txt') }}
            -${{ hashFiles('**/requirements-extras.txt') }}-${{ hashFiles('**/CI.yml') }}-${{ hashFiles('setup.py') }}
            -${{ steps.date.outputs.date }}

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install coveralls
          pip install pytest-benchmark
          pip install -r requirements/requirements-tests.txt
          pip install -r requirements/requirements.txt
          pip install -r requirements/requirements-extras.txt
          pip install -e .

      - name: List packages
        run: |
          pip list
          python --version

      - name: Benchmark with pytest-benchmark
        run: |
          # pytest --benchmark-only --benchmark-compare --benchmark-autosave
          pytest --benchmark-only --benchmark-json output.json
        env:
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

      - name: Store benchmark result
        uses: benchmark-action/github-action-benchmark@v1
        with:
          tool: 'pytest'
          output-file-path: output.json
          github-token: ${{ secrets.GITHUB_TOKEN }}
          # Push and deploy GitHub pages branch automatically
          auto-push: true
